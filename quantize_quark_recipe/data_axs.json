{
    "_parent_entries": [ [ "^", "byname", "python_script" ], [ "^", "byname", "entry_creator" ] ],
    "_producer_rules": [
        [ [ "quantized", "method=quark" ], [[ "get", "pipeline" ]] ],

        [ [ "git_repo", "repo_name=quark" ], [ [ "get_kernel" ], [ "byname", "git" ], [ "clone" ] ], {
            "url": "https://github.com/amd/Quark",
            "checkout": "release/0.8"
        } ],

        [ [ "python_package", "contains_deps", "package_name=quantize_quark_python_deps" ],
            [ [ "get_kernel" ], [ "byname", "pip" ], [ "install" ] ], {
                "path_to_requirements": [ "^^", "execute", [[
                    [ "get", "quark_source_entry" ],
                    [ "get_path", "examples/torch/language_modeling/llm_ptq/requirements.txt" ]
                ]] ],
                "installable": [ "AS^IS", "^^", "substitute", "-r #{path_to_requirements}#" ]
            }, [ "quark_source_entry" ]
        ],

        [ [ "python_package", "package_name=torch", "target=rocm" ],
            [ [ "get_kernel" ], [ "byname" , "pip" ], [ "install" ] ], {
                "pip_options": "--no-cache-dir --index-url https://download.pytorch.org/whl/rocm6.3",
                "newborn_name_template": "#{package_name}##{version_suffix_name}#_package_for_python#{python_major_dot_minor}#-#{target}#"
            }
        ]
    ],

    "pipeline": [ "^^", "execute", [[
        [ "run" ],
        [ ],
        [ "get", "stored_newborn_entry" ]
    ]] ], 

    "model_family": "llama3_1",
    "model_variant": "8b",

    "model_query": [ "^^", "substitute", [[
        "downloaded",
        "hf_model",
        [ "model_family", "#{model_family}#" ],
        [ "variant", "#{model_variant}#" ]
    ]] ],
    "model_entry": [ "^", "byquery", [[ "^^", "get", "model_query" ]], {}, [ "model_query" ] ],
    "model_path": [ "^^", "execute", [[
        [ "get", "model_entry" ],
        [ "get_path" ]
    ]] ],
    
    "device":                           "auto",
    "model_attn_implementation":        "eager",
    "data_type":                        "auto",
    "max_seq_len":                      512,
    "batch_size":                       1,
    "group_size":                       128,
    "quant_scheme":                     "w_fp8_a_fp8",
    "kv_cache_dtype":                   "fp8",
    "min_kv_scale":                     0.0,
    "model_export":                     "hf_format",
    "pack_method":                      "reorder",
    "weight_matrix_merge":              false,
    "export_weight_format":             "real_quantized",
    "fp8_attention_quant":              false,
    "exclude_layers":                   "",
    "pre_quantization_optimization":    "",
    "quant_algo":                       "",
    "group_size_per_layer":             "",
    "custom_mode":                      "quark",

    "model_type": [ "^^", "case", [[ "^^", "get", "model_family" ],
        "llama2", "llama",
        "llama3_1", "llama"
    ]],

    "pre_optimization_config_file_path": [ "^^", "case", [[ "^^", "get", "pre_quantization_optimization" ],
        "", "",
        "rotation", "",
        "quarot", [ "^^", "substitute", "#{quark_models_path}#/quarot_config.json" ],
        "smoothquant", [ "^^", "substitute", "#{quark_models_path}#/smooth_config.json" ]
    ]],

    "quant_algo_config_file_path": [ "^^", "case", [[ "^^", "get", "quant_algo" ],
        "", "",
        "awq", [ "^^", "substitute", "#{quark_models_path}#/awq_config.json" ],
        "gptq", [ "^^", "substitute", "#{quark_models_path}#/gptq_config.json" ],
        "autosmoothquant", [ "^^", "substitute", "#{quark_models_path}#/autosmoothquant_config.json" ]
    ]],

    "num_calib_data": [ "^^", "case", [[ "^^", "substitute", "#{model_family}#-#{model_variant}#" ],
        "llama2-70b", 1000
    ]],

    "dataset_name": [ "^^", "case", [[ "^^", "substitute", "#{model_family}#-#{model_variant}#" ],
        "llama2-70b", "openorca"
    ]],

    "dataset_local_path": [ "^^", "case", [[ "^^", "get", "dataset_name" ],
        "openorca", "open_orca_gpt4_tokenized_llama.calibration_1000.pkl"
    ]],

    "dataset_query": [ "^^", "substitute", [[
        "downloaded",
        [ "dataset_name", "#{dataset_name}#" ],
        [ "model_family", "#{model_family}#" ],
        [ "variant", "#{model_variant}#"]
    ]] ],
    "dataset_entry": [ "^", "byquery", [[ "^^", "get", "dataset_query" ]], {}, [ "dataset_query" ] ],
    "dataset_path": [ "^^", "execute", [[
        [ "get", "dataset_entry" ],
        [ "get_path", [ "^^", "get", "dataset_local_path" ] ]
    ]] ],

    "desired_python_version": "3.10",

    "quark_source_entry": [ "^", "byquery", "git_repo,repo_name=quark" ],
    "quark_source_path": [ "^^", "execute", [[
        [ "get", "quark_source_entry" ],
        [ "get_path" ]
    ]] ],
    "quark_models_path": [ "^^", "execute", [[
        [ "get", "quark_source_entry" ],
        [ "get_path",  [ "^^", "substitute", "examples/torch/language_modeling/llm_ptq/models/#{model_type}#" ] ]
    ]] ],
 
    "python_deps": [
        [ "^^", "python_sync_pip_package", [[ "python_package", "package_name=torch", "target=rocm" ]] ],
        [ "^^", "python_sync_pip_package", [[ "python_package", "package_name=nltk" ]] ],
        [ "^^", "python_sync_pip_package", [[ "python_package", "package_name=amd-quark" ]] ],
        [ "^^", "python_sync_pip_package", [[ "python_package", "contains_deps", "package_name=quantize_quark_python_deps" ]] ]
    ],

    "newborn_entry_tags": [ "quantized", "method=quark" ],
    "newborn_name_template": [ "quantized_using_quark_model_#{model_family}#_#{model_variant}#" ],
    "newborn_entry_param_names": [
        "model_family",
        "model_variant",
        "model_path",
        "max_seq_len",
        "batch_size",
        "num_calib_data",
        "device",
        "data_type",
        "model_attn_implementation",
        "dataset_name",
        "dataset_path",
        "quark_source_path",
        "quant_scheme",
        "group_size",
        "kv_cache_dtype",
        "fp8_attention_quant",
        "exclude_layers",
        "pre_quantization_optimization",
        "pre_optimization_config_file_path",
        "quant_algo",
        "quant_algo_config_file_path",
        "model_type",
        "group_size_per_layer",
        "weight_matrix_merge",
        "export_weight_format",
        "pack_method",
        "min_kv_scale",
        "custom_mode"
    ],

    "extra_env": {
	"CPLUS_INCLUDE_PATH": "/usr/include/c++/11:/usr/include/x86_64-linux-gnu/c++/11"
    },

    "rel_script_path": "quantize.py",

    "script_extra_params": [ "^^", "substitute", "#{newborn_entry_path}#" ]
}
